---
title: "Final Project"
author: "Amrita Acharya & Sunni Raleigh"
date: "4/10/2022"
output: html_document
---

## LOAD PACKAGES
```{r include=FALSE}
library(tidyverse)
library(ggplot2)
# library(randomForest)
library(ggcorrplot)

set.seed(1)
```

## LOAD DATA
```{r}
data <- read.csv("colorado_housing_data.csv") %>%
  mutate(FOODSTMP = as.factor(ifelse(FOODSTMP == 1, 1, 0))) %>%
  select(-c(X, YEAR, HHWT, CLUSTER, GQ, STATEFIP, STRATA, PERNUM, PERWT, EMPSTATD, VETSTATD))
```

## QUESTION 
What are the most important determinants for someone to be eligible for food stamps in the state of Colorado? 

Which variables will predict the likelihood of someone receiving food stamps or not in Colorado?

Questions for Sanogo:
- We're interested in tree and logit regression for models, what are some other models we could use for classification?  
- What are all the model choosing methods we could use? We were thinking of using CV and lasso. Tuning for trees?
- Feedback on research question.

## PART 1: EXPLORATORY DATA ANALYSIS

### Variable list
- `RENT`: Monthly contract rent  
- `FOODSTMP`: Food stamp recipiency  
- `VALUEH`: House value  
- `RACNUM`: Number of major race groups  
- `HCOVANY`: Any health insurance coverage  
- `HCOVPUB`: Public health insurance coverage  
- `HINSIHS`: Health insurance through Indian Health Services  
- `EMPSTAT`: Employment status  
- `OCC`: Occupation  
- `VETSTAT`: Veteran status  
- `TRANWORK`: Means of transportation to work  
- `TRANTIME`: Travel time to work  

### Summary of data
```{r}
summary(data)
```

### Distribution of response variable
```{r}
count <- data %>%
  group_by(FOODSTMP) %>%
  summarise(total = n())

ggplot(count, aes(x = FOODSTMP, y = total, fill = FOODSTMP)) +
  geom_col(show.legend = FALSE) +
  geom_label(aes(label = total), show.legend = FALSE)
```

Important to notice that most of the people receive food stamps.

#### Relationship between rent and receiving foodstamps
```{r}
ggplot(data, aes(y = RENT, x = FOODSTMP)) +
  geom_boxplot()
```

#### Relationship between transit time and receiving foodstamps
```{r}
ggplot(data, aes(y = TRANTIME, x = FOODSTMP)) +
  geom_boxplot()
```

#### Correlation of variables
```{r}
model.matrix(~0+., data = data) %>%
  cor(use = "pairwise.complete.obs") %>%
  ggcorrplot(lab = TRUE, lab_size = 2)
```
Notes: Rent and house value are pretty correlated.

## PART 2: METHODOLOGY
We split the data into a training set and a testing set.

```{r}
train <- data %>%
  dplyr::sample_frac(0.8)

test <- data %>%
  anti_join(train)
```


### MODEL 1: LOGISTIC REGRESSION

We performed a logistic regression on our data first to understand the probabilities of someone getting food stamps in Colorado. We wanted to use logistic regression first because it is easy to interpret for the sake of this research project, and it is a model we are very familiar with. Our food stamp variable FOODSTMP has a binary response, either 0, not receiving food stamps, or 1, receiving food stamps. Becaues of this, we decided on logistic regression because it works with binary responses. Logistic regression also has low variance because it assumes a linear relationship between the predictor and resposne.

#### Model for entire dataset

```{r}
glm_fit <- glm(FOODSTMP~.,
              data = data,
              family = binomial)
summary(glm_fit)
```

```{r}
glm_probs <- data.frame(probs = predict(glm_fit, type="response"))
head(glm_probs)
```

#### Training model

```{r}
glm_2 <- glm(FOODSTMP~., data = train, family = binomial)
summary(glm_2)
```

```{r}
glm_probs2 <- data.frame(probs = predict(glm_2, newdata = test, type="response"))
head(glm_probs2)
```

```{r}
#create probabilities

glm_pred <- glm_probs2 %>%
  mutate(pred = ifelse(probs>.5, "No", "Yes"))

glm_pred <- cbind(test, glm_pred)

glm_pred %>% 
  count(pred, FOODSTMP) %>%
  spread(FOODSTMP, n, fill = 0)

glm_pred %>%
  summarize(score = mean(pred == FOODSTMP))
```

### MODEL 2: BAGGING

Based on recommendations from Professor Sanogo, we decided to also use tree based methods for its interpretability and because our dataset has many predictors. We decided on bagging because it reduces variance in our data. 

Is the computational power worth it?? 


#### Bagging Model 
https://discuss.analyticsvidhya.com/t/what-does-the-warning-the-response-has-five-or-fewer-unique-values-while-building-random-forest-mean/6442

```{r}
# bagging_model <- randomForest(FOODSTMP~.,
#                          data = train,
#                          mtry = 11,
#                          importance = TRUE
#                          )
# 
# saveRDS(bagging_model, "bag_mod.rds")

bagging_model <- readRDS("bag_mod.rds")

summary(bagging_model)
```

```{r}
bagging_estimate <- predict(bagging_model, newdata = train)
mean((bagging_estimate - test$FOODSTMP)^2)
```


## PART 3: KEY TAKEAWAYS 

